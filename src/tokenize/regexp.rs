//! Regular-Expression Tokenizers
//!
//! A ``RegexpTokenizer`` splits a string into substrings using a regular expression.
//! For example, the following tokenizer forms tokens out of alphabetic sequences,
//! money expressions, and any other non-whitespace sequences:
//!
//! **example**
//!
//! A ``RegexpTokenizer`` can use its regexp to match delimiters instead:
//!
//! **example**
//!
//! Note that empty tokens are not returned when the delimiter appears at
//! the start or end of the string.
//!
//! The material between the tokens is discarded.  For example,
//! the following tokenizer selects just the capitalized words:
//!
//! **example**
//!
//! This module contains several subclasses of ``RegexpTokenizer``
//! that use pre-defined regular expressions.
//!
//! **example**
//!
//! All of the regular expression tokenizers are also available as functions:
//!
//! **example**
//!
//! Caution: The function ``regexp_tokenize()`` takes the text as its
//! first argument, and the regular expression pattern as its second
//! argument.  This differs from the conventions used by Python's
//! ``re`` functions, where the pattern is always the first argument.
//! (This is for consistency with the other NLTK tokenizers.)

use super::api::TokenizerI;
use super::util::Token;

use regex::{Regex, RegexBuilder};

/// A tokenizer that splits a string using a regular expression, which
/// matches either the tokens or the separators between tokens.
///
/// All text are encoded in UTF-8 in Rust, so reason to assume otherwise.
///
/// # Examples
///
/// tokenizer = RegexpTokenizer::new('\w+|\$[\d\.]+|\S+');
///
#[derive(Debug)]
pub struct RegexpTokenizer<'a> {
    pattern: &'a str,
    regex: Option<Regex>,
    gaps: bool,
    discard_empty: bool,
    multiline: bool,
    match_newline: bool,
    case_insensitive: bool,
}
impl<'a> TokenizerI<'a> for RegexpTokenizer<'a> {
    fn tokenize(&self, sent: &'a str) -> Vec<&'a str> {
        let mut result: Vec<&str> = Vec::new();
        let mut index = 0;
        let matches = self
            .regex
            .as_ref()
            .expect("The regex must be built before you can use it.")
            .find_iter(sent);
        for m in matches {
            result.push(sent.get(index..m.start()).unwrap());
            index = m.end();
        }

        result
    }
    fn span_tokenize(&self, sent: &str) -> Vec<Token> {
        let mut result: Vec<Token> = Vec::new();
        let mut index: usize = 0;
        let matches = self
            .regex
            .as_ref()
            .expect("The regex must be built before you can use it.")
            .find_iter(sent);
        for m in matches {
            result.push((index, m.start()));
            index = m.end();
        }
        result.push((index, sent.len()));

        result
    }
}
impl<'a> RegexpTokenizer<'a> {
    /// Initialize a [RegexpTokenizer] for a given pattern
    ///
    /// NOTE: you NEED to compile the given pattern before you can use
    /// Failure to compiling will throw an Panic
    pub fn new(ptrn: &'a str) -> Self {
        Self {
            pattern: ptrn,
            multiline: true,
            match_newline: true,
            case_insensitive: false,
            regex: None,
            gaps: false,
            discard_empty: false,
        }
    }
    /// True if this tokenizer's pattern should be used
    /// to find separators between tokens; False if this
    /// tokenizer's pattern should be used to find the tokens
    /// themselves.
    pub fn set_gaps(mut self, yes: bool) -> Self {
        self.gaps = yes;
        self
    }
    /// True if any empty tokens `''`
    /// generated by the tokenizer should be discarded.  Empty
    /// tokens can only be generated if `_gaps == True`.
    pub fn set_discard_empty(mut self, yes: bool) -> Self {
        self.discard_empty = yes;
        self
    }
    /// Sets if matches between multiple lines
    ///
    /// Defaults to 'TRUE' - will match between multiple lines
    pub fn set_multiline(mut self, yes: bool) -> Self {
        self.multiline = yes;
        self
    }
    /// Sets if the any character will match including the newline character
    ///
    /// Defaults to 'TRUE' - matches include the newline character
    pub fn set_match_newline(mut self, yes: bool) -> Self {
        self.match_newline = yes;
        self
    }
    /// Sets if matches are case sensitive or not
    ///
    /// Defaults to 'FALSE' - matches are case sensitive
    pub fn set_case_insensitive(mut self, yes: bool) -> Self {
        self.case_insensitive = yes;
        self
    }
    /// Builds the Regex pattern.
    ///
    /// Panics if unable to compile the pattern for any reason.
    pub fn build(mut self) -> Self {
        let mut builder = RegexBuilder::new(self.pattern);
        builder
            .unicode(true)
            .multi_line(self.multiline)
            .dot_matches_new_line(self.match_newline)
            .case_insensitive(self.case_insensitive);
        match builder.build() {
            Ok(good) => self.regex = Some(good),
            Err(e) => {
                panic!("{:?}", e)
            }
        }
        self
    }
}

/// Tokenize a string on whitespace (space, tab, newline).
/// In general, users should use the [std::str::SplitWhitespace] method instead.
#[allow(non_snake_case)]
pub fn WhitespaceTokenizer() -> RegexpTokenizer<'static> {
    RegexpTokenizer::new(r"\s+").build()
}

/// Tokenize a string, treating any sequence of blank lines as a delimiter.
/// Blank lines are defined as lines containing no characters, except for
/// space or tab characters.
#[allow(non_snake_case)]
pub fn BlanklineTokenizer() -> RegexpTokenizer<'static> {
    RegexpTokenizer::new(r"\s*\n\s*\n\s*").build()
}

/// Tokenize a text into a sequence of alphabetic and
/// non-alphabetic characters, using the regexp ``\w+|[^\w\s]+``.
#[allow(non_snake_case)]
pub fn WordPunctTokenizer() -> RegexpTokenizer<'static> {
    RegexpTokenizer::new(r"\w+|[^\w\s]+").build()
}
